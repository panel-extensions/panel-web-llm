{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u2728 panel-web-llm","text":"<p>This extension for HoloViz Panel introduces a client-side interface for running large language models (LLMs) directly in the browser.</p> <p>It leverages WebLLM under the hood to provide an in-browser LLM execution environment, enabling fully client-side interactions without relying on server-side APIs.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Run LLMs in the Browser: Execute large language models directly in the browser without requiring server-side APIs or cloud services.</li> <li>Offline Capability: Cache models locally in the browser, enabling offline use after initial download.</li> <li>Model Variety: Supports multiple models, including Llama 2 and Qwen 2.5. Check Available Models for the most up-to-date list.</li> <li>Privacy-Preserving: Keeps all computations client-side, ensuring data privacy and security.</li> <li>Panel Integration: Effortlessly incorporate LLM-powered features into interactive Panel applications.</li> </ul>"},{"location":"#pin-version","title":"Pin Version","text":"<p>This project is in its early stages, so if you find a version that suits your needs, it\u2019s recommended to pin your version, as updates may introduce changes.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install it via <code>pip</code>:</p> <pre><code>pip install panel-web-llm\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#online","title":"Online","text":"<p>Try it out in Examples.</p>"},{"location":"#command-line-interface","title":"Command Line Interface","text":"<p>Once installed, you may launch the web LLM in the terminal with the following command:</p> <pre><code>panel-web-llm\n</code></pre> <p>Once the server launches, the <code>Load &lt;model_name&gt;</code> button has been clicked, the model is cached in your browser.</p> <p>That means, even if you restart the server without internet, you can still run that same model offline, as long as your browser cache is not cleared.</p> <p>The following is an alias for convenience:</p> <pre><code>pllm\n</code></pre> <p>The default model used is <code>Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC</code>. To default to another model:</p> <pre><code>panel-web-llm run &lt;model_name&gt;\n</code></pre> <p>Replace <code>&lt;model_name&gt;</code> with the name of the model you want to use. For a list of models:</p> <pre><code>panel-web-llm list\n</code></pre>"},{"location":"#python","title":"Python","text":"<p>You can seamlessly integrate the Web LLM interface into your Panel applications:</p> <pre><code>import panel as pn\nfrom panel_web_llm import WebLLMInterface\npn.extension()\n\ntemplate = pn.template.FastListTemplate(\n    title=\"Web LLM Interface\", main=[WebLLMInterface()]\n)\ntemplate.show()\n</code></pre> <p>If you don't like the built-in layout of <code>WebLLMInterface</code>, you can instead wrap <code>WebLLM</code> manually:</p> <pre><code>import panel as pn\nfrom panel_web_llm import WebLLM\n\npn.extension()\n\nweb_llm = WebLLM(load_layout=\"column\")\nchat_interface = pn.chat.ChatInterface(\n    callback=web_llm.callback,\n)\n\ntemplate = pn.template.FastListTemplate(\n    title=\"Web LLM Interface\",\n    main=[chat_interface],\n    sidebar=[web_llm.menu, web_llm],  # important to include `web_llm`\n    sidebar_width=350,\n)\ntemplate.show()\n</code></pre>"},{"location":"#development","title":"Development","text":"<pre><code>git clone https://github.com/panel-extensions/panel-web-llm\ncd panel-web-llm\n</code></pre> <p>For a simple setup use <code>uv</code>:</p> <pre><code>uv venv\nsource .venv/bin/activate # on linux. Similar commands for windows and osx\nuv pip install -e .[dev]\npre-commit run install\npytest tests\n</code></pre> <p>For the full Github Actions setup use pixi:</p> <pre><code>pixi run pre-commit-install\npixi run postinstall\npixi run test\n</code></pre> <p>This repository is based on copier-template-panel-extension (you can create your own Panel extension with it)!</p> <p>To update to the latest template version run:</p> <pre><code>pixi exec --spec copier --spec ruamel.yaml -- copier update --defaults --trust\n</code></pre> <p>Note: <code>copier</code> will show <code>Conflict</code> for files with manual changes during an update. This is normal. As long as there are no merge conflict markers, all patches applied cleanly.</p>"},{"location":"#contributing","title":"\u2764\ufe0f Contributing","text":"<p>Contributions are welcome! Please follow these steps to contribute:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch: <code>git checkout -b feature/YourFeature</code>.</li> <li>Make your changes and commit them: <code>git commit -m 'Add some feature'</code>.</li> <li>Push to the branch: <code>git push origin feature/YourFeature</code>.</li> <li>Open a pull request.</li> </ol> <p>Please ensure your code adheres to the project's coding standards and passes all tests.</p>"},{"location":"examples/","title":"Examples","text":"<pre><code>import panel as pn\nfrom panel_web_llm import WebLLMInterface\n\npn.extension()\n\nweb_llm_interface = WebLLMInterface()\nweb_llm_interface.servable()\n</code></pre> <pre><code>import panel as pn\nfrom panel_web_llm import WebLLM\n\npn.extension()\n\nweb_llm = WebLLM(load_layout=\"column\")\nchat_interface = pn.chat.ChatInterface(\n    help_text=\"First load a model to chat with it.\",\n    callback=web_llm.callback,\n    show_button_name=False,\n    show_rerun=False,\n    show_undo=False,\n    show_clear=False,\n)\ntemplate = pn.template.FastListTemplate(\n    title=\"Web LLM Interface\",\n    main=[chat_interface],\n    sidebar=[web_llm.menu, web_llm],  # important to include `web_llm`\n    sidebar_width=350,\n)\ntemplate.servable()\n</code></pre>"},{"location":"reference/panel_web_llm/","title":"Reference","text":"<p>Accessible imports for the panel_web_llm package.</p>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam","title":"<code>ModelParam</code>","text":"<p>               Bases: <code>Parameterized</code></p> <p>A class to represent model parameters including model name, size, and quantization.</p> <p>This class provides methods to lookup model slugs based on mappings, convert parameters to dictionaries, and create instances from nested selects or model slugs.</p> Source code in <code>src/panel_web_llm/models.py</code> <pre><code>class ModelParam(param.Parameterized):\n    \"\"\"A class to represent model parameters including model name, size, and quantization.\n\n    This class provides methods to lookup model slugs based on mappings, convert parameters\n    to dictionaries, and create instances from nested selects or model slugs.\n    \"\"\"\n\n    model = param.String(doc=\"The model name.\")\n    \"\"\"The name of the model.\"\"\"\n\n    size = param.String(doc=\"The size of the model.\")\n    \"\"\"The size of the model, e.g., '7B', '13B', etc.\"\"\"\n\n    quantization = param.String(doc=\"The quantization of the model.\")\n    \"\"\"The quantization applied to the model, e.g., 'AWQ', 'GPTQ'.\"\"\"\n\n    def lookup_model_slug(self, model_mapping: dict) -&gt; str:\n        \"\"\"Looks up the model slug based on the given model mapping.\n\n        Args:\n            model_mapping (dict): A nested dictionary mapping model name, size, and quantization to a model slug.\n\n        Returns\n        -------\n            str: The model slug corresponding to the current model parameters.\n        \"\"\"\n        return model_mapping[self.model][self.size][self.quantization]\n\n    def to_dict(self, levels: list) -&gt; dict:\n        \"\"\"Converts the model parameters to a dictionary.\n\n        Args:\n            levels (list): A list of keys or dictionaries defining how the parameters should be structured in the output dictionary.\n                           If the levels are strings they will be used as keys. If the levels are dictionaries, the 'name' key\n                           will be used as key.\n\n        Returns\n        -------\n            dict: A dictionary representation of the model parameters.\n        \"\"\"\n        if not isinstance(levels[0], dict):\n            return {\n                levels[0]: self.model,\n                levels[1]: self.size,\n                levels[2]: self.quantization,\n            }\n        return {\n            levels[0][\"name\"]: self.model,\n            levels[1][\"name\"]: self.size,\n            levels[2][\"name\"]: self.quantization,\n        }\n\n    @classmethod\n    def from_nested_select(cls, nested_select: pn.widgets.NestedSelect) -&gt; \"ModelParam\":\n        \"\"\"Creates a ModelParam instance from a NestedSelect widget.\n\n        Args:\n            nested_select (pn.widgets.NestedSelect): The NestedSelect widget containing the model parameters.\n\n        Returns\n        -------\n            ModelParam: A ModelParam instance initialized with the values from the NestedSelect.\n        \"\"\"\n        value = nested_select.value\n        levels = nested_select.levels\n        if not isinstance(levels[0], dict):\n            return cls(\n                model=value[levels[0]],\n                size=value[levels[1]],\n                quantization=value[levels[2]],\n            )\n        return cls(\n            model=value[levels[0][\"name\"]],\n            size=value[levels[1][\"name\"]],\n            quantization=value[levels[2][\"name\"]],\n        )\n\n    @classmethod\n    def from_model_slug(cls, model_slug: str) -&gt; \"ModelParam\":\n        \"\"\"Creates a ModelParam instance from a model slug string.\n\n        Args:\n            model_slug (str): The model slug string, e.g., 'llama-7B-AWQ'.\n\n        Returns\n        -------\n            ModelParam: A ModelParam instance initialized with the parsed values from the model slug.\n        \"\"\"\n        model_label, model_quantization, _ = model_slug.rsplit(\"-\", 2)\n        model_parameters_re = re.search(r\"\\d+(\\.\\d+)?[BbKkmM]\", model_label)\n        if model_parameters_re:\n            model_parameters = model_parameters_re.group(0)\n            model_name = model_label[: model_parameters_re.start()].rstrip(\"-\").rstrip(\"_\")\n        else:\n            model_parameters = \"-\"\n            model_name = model_label.rstrip(\"-\").rstrip(\"_\")\n        return cls(model=model_name, size=model_parameters, quantization=model_quantization)\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.model","title":"<code>model = param.String(doc='The model name.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the model.</p>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.quantization","title":"<code>quantization = param.String(doc='The quantization of the model.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The quantization applied to the model, e.g., 'AWQ', 'GPTQ'.</p>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.size","title":"<code>size = param.String(doc='The size of the model.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The size of the model, e.g., '7B', '13B', etc.</p>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.from_model_slug","title":"<code>from_model_slug(model_slug)</code>  <code>classmethod</code>","text":"<p>Creates a ModelParam instance from a model slug string.</p> <p>Parameters:</p> Name Type Description Default <code>model_slug</code> <code>str</code> <p>The model slug string, e.g., 'llama-7B-AWQ'.</p> required"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.from_model_slug--returns","title":"Returns","text":"<pre><code>ModelParam: A ModelParam instance initialized with the parsed values from the model slug.\n</code></pre> Source code in <code>src/panel_web_llm/models.py</code> <pre><code>@classmethod\ndef from_model_slug(cls, model_slug: str) -&gt; \"ModelParam\":\n    \"\"\"Creates a ModelParam instance from a model slug string.\n\n    Args:\n        model_slug (str): The model slug string, e.g., 'llama-7B-AWQ'.\n\n    Returns\n    -------\n        ModelParam: A ModelParam instance initialized with the parsed values from the model slug.\n    \"\"\"\n    model_label, model_quantization, _ = model_slug.rsplit(\"-\", 2)\n    model_parameters_re = re.search(r\"\\d+(\\.\\d+)?[BbKkmM]\", model_label)\n    if model_parameters_re:\n        model_parameters = model_parameters_re.group(0)\n        model_name = model_label[: model_parameters_re.start()].rstrip(\"-\").rstrip(\"_\")\n    else:\n        model_parameters = \"-\"\n        model_name = model_label.rstrip(\"-\").rstrip(\"_\")\n    return cls(model=model_name, size=model_parameters, quantization=model_quantization)\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.from_nested_select","title":"<code>from_nested_select(nested_select)</code>  <code>classmethod</code>","text":"<p>Creates a ModelParam instance from a NestedSelect widget.</p> <p>Parameters:</p> Name Type Description Default <code>nested_select</code> <code>NestedSelect</code> <p>The NestedSelect widget containing the model parameters.</p> required"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.from_nested_select--returns","title":"Returns","text":"<pre><code>ModelParam: A ModelParam instance initialized with the values from the NestedSelect.\n</code></pre> Source code in <code>src/panel_web_llm/models.py</code> <pre><code>@classmethod\ndef from_nested_select(cls, nested_select: pn.widgets.NestedSelect) -&gt; \"ModelParam\":\n    \"\"\"Creates a ModelParam instance from a NestedSelect widget.\n\n    Args:\n        nested_select (pn.widgets.NestedSelect): The NestedSelect widget containing the model parameters.\n\n    Returns\n    -------\n        ModelParam: A ModelParam instance initialized with the values from the NestedSelect.\n    \"\"\"\n    value = nested_select.value\n    levels = nested_select.levels\n    if not isinstance(levels[0], dict):\n        return cls(\n            model=value[levels[0]],\n            size=value[levels[1]],\n            quantization=value[levels[2]],\n        )\n    return cls(\n        model=value[levels[0][\"name\"]],\n        size=value[levels[1][\"name\"]],\n        quantization=value[levels[2][\"name\"]],\n    )\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.lookup_model_slug","title":"<code>lookup_model_slug(model_mapping)</code>","text":"<p>Looks up the model slug based on the given model mapping.</p> <p>Parameters:</p> Name Type Description Default <code>model_mapping</code> <code>dict</code> <p>A nested dictionary mapping model name, size, and quantization to a model slug.</p> required"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.lookup_model_slug--returns","title":"Returns","text":"<pre><code>str: The model slug corresponding to the current model parameters.\n</code></pre> Source code in <code>src/panel_web_llm/models.py</code> <pre><code>def lookup_model_slug(self, model_mapping: dict) -&gt; str:\n    \"\"\"Looks up the model slug based on the given model mapping.\n\n    Args:\n        model_mapping (dict): A nested dictionary mapping model name, size, and quantization to a model slug.\n\n    Returns\n    -------\n        str: The model slug corresponding to the current model parameters.\n    \"\"\"\n    return model_mapping[self.model][self.size][self.quantization]\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.to_dict","title":"<code>to_dict(levels)</code>","text":"<p>Converts the model parameters to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>levels</code> <code>list</code> <p>A list of keys or dictionaries defining how the parameters should be structured in the output dictionary.            If the levels are strings they will be used as keys. If the levels are dictionaries, the 'name' key            will be used as key.</p> required"},{"location":"reference/panel_web_llm/#panel_web_llm.ModelParam.to_dict--returns","title":"Returns","text":"<pre><code>dict: A dictionary representation of the model parameters.\n</code></pre> Source code in <code>src/panel_web_llm/models.py</code> <pre><code>def to_dict(self, levels: list) -&gt; dict:\n    \"\"\"Converts the model parameters to a dictionary.\n\n    Args:\n        levels (list): A list of keys or dictionaries defining how the parameters should be structured in the output dictionary.\n                       If the levels are strings they will be used as keys. If the levels are dictionaries, the 'name' key\n                       will be used as key.\n\n    Returns\n    -------\n        dict: A dictionary representation of the model parameters.\n    \"\"\"\n    if not isinstance(levels[0], dict):\n        return {\n            levels[0]: self.model,\n            levels[1]: self.size,\n            levels[2]: self.quantization,\n        }\n    return {\n        levels[0][\"name\"]: self.model,\n        levels[1][\"name\"]: self.size,\n        levels[2][\"name\"]: self.quantization,\n    }\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM","title":"<code>WebLLM</code>","text":"<p>               Bases: <code>JSComponent</code></p> <p>A Panel component for interacting with WebLLM models.</p> <p>This component provides an interface to load and interact with models served by the <code>web-llm</code> library. It manages model loading, chat completion, and provides controls for model selection and temperature.</p> Source code in <code>src/panel_web_llm/main.py</code> <pre><code>class WebLLM(JSComponent):\n    \"\"\"\n    A Panel component for interacting with WebLLM models.\n\n    This component provides an interface to load and interact with models served\n    by the `web-llm` library. It manages model loading, chat completion,\n    and provides controls for model selection and temperature.\n    \"\"\"\n\n    history = param.Integer(\n        default=10,\n        bounds=(1, 100),\n        doc=\"The number of previous messages to include in the completion.\",\n    )\n\n    load_status = param.Dict(default={\"text\": \"\", \"progress\": 0})\n\n    load_model = param.Event(doc=\"Event to trigger model loading.\")\n\n    load_layout = param.Selector(\n        default=\"row\",\n        objects=[\"row\", \"column\"],\n        doc=\"\"\"\n        The layout type of the widgets.\"\"\",\n    )\n\n    model_slug = param.Selector(default=\"\", doc=\"The model slug to load.\")\n\n    model_mapping = param.Dict(default=MODEL_MAPPING, doc=\"Nested mapping of model names to slugs.\")\n\n    multiple_loads = param.Boolean(\n        default=True,\n        doc=\"Whether to allow loading different models multiple times\",\n    )\n\n    running = param.Boolean(\n        default=False,\n        doc=\"Whether the LLM is currently running.\",\n    )\n\n    system = param.String(\n        default=\"Be the most you can be; spread positivity!\",\n        doc=\"The system prompt for the model completion.\",\n    )\n\n    temperature = param.Number(\n        default=1,\n        bounds=(0, 2),\n        doc=\"The temperature for the model completion.\",\n    )\n\n    loaded = param.Boolean(\n        default=False,\n        doc=\"Whether the model is loaded.\",\n    )\n\n    loading = param.Boolean(\n        default=False,\n        doc=\"\"\"\n        Whether the model is currently loading.\"\"\",\n    )\n\n    _esm = \"\"\"\n        import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\n\n        const engines = new Map()\n\n        export async function render({ model }) {\n        model.on(\"msg:custom\", async (event) =&gt; {\n            if (event.type === 'load') {\n            model.loading = true\n            if (!engines.has(model.model_slug)) {\n                const initProgressCallback = (load_status) =&gt; {\n                model.load_status = load_status\n                }\n                try {\n                const mlc = await webllm.CreateMLCEngine(\n                    model.model_slug,\n                    { initProgressCallback }\n                )\n                engines.set(model.model_slug, mlc)\n                model.loaded = true\n                } catch (error) {\n                model.load_status = {\n                    progress: 0,\n                    text: error.message + \" Try again later, or try a different size/quantization.\",\n                };\n                model.loaded = false;\n                }\n            }\n            model.loading = false\n            } else if (event.type === 'completion') {\n            const engine = engines.get(model.model_slug)\n            if (engine == null) {\n                model.send_msg({ 'finish_reason': 'error' })\n            }\n            const chunks = await engine.chat.completions.create({\n                messages: event.messages,\n                temperature: model.temperature,\n                stream: true,\n            })\n            model.running = true\n            for await (const chunk of chunks) {\n                if (!model.running) {\n                break\n                }\n                model.send_msg(chunk.choices[0])\n            }\n            }\n        })\n        }\n    \"\"\"\n\n    def __init__(self, **params):\n        \"\"\"\n        Initializes the WebLLM component.\n\n        Args:\n            **params:\n                Keyword arguments for the Panel and Param base classes.\n        \"\"\"\n        self._buffer = []\n        load_layout = pn.Column if params.get(\"load_layout\") == \"column\" else pn.Row\n        self._model_select = pn.widgets.NestedSelect(layout=load_layout)\n        super().__init__(**params)\n\n        self._history_input = pn.widgets.IntSlider.from_param(\n            self.param.history,\n            disabled=self.param.loading,\n            sizing_mode=\"stretch_width\",\n        )\n        self._temperature_input = pn.widgets.FloatSlider.from_param(\n            self.param.temperature,\n            disabled=self.param.loading,\n            sizing_mode=\"stretch_width\",\n        )\n        self._load_button = pn.widgets.Button.from_param(\n            self.param.load_model,\n            name=param.rx(\"Load \") + self.param.model_slug,\n            loading=self.param.loading,\n            align=(\"start\", \"end\"),\n            button_type=\"primary\",\n            description=None,  # override default text\n        )\n        load_status = self.param.load_status.rx()\n        load_row = load_layout(\n            self._model_select,\n            self._load_button,\n            sizing_mode=\"stretch_width\",\n            margin=0,\n        )\n        config_row = pn.Row(\n            self._temperature_input,\n            self._history_input,\n            sizing_mode=\"stretch_width\",\n            margin=0,\n        )\n        system_input = pn.widgets.TextAreaInput.from_param(\n            self.param.system,\n            auto_grow=True,\n            resizable=\"height\",\n            sizing_mode=\"stretch_width\",\n        )\n        load_progress = pn.Column(\n            pn.indicators.Progress(\n                value=(load_status[\"progress\"] * 100).rx.pipe(int),\n                visible=self.param.loading,\n                sizing_mode=\"stretch_width\",\n                margin=(5, 10, -10, 10),\n                height=30,\n            ),\n            pn.pane.HTML(\n                load_status[\"text\"],\n                visible=load_status.rx.len() &gt; 0,\n            ),\n        )\n        self._card_header = pn.pane.HTML(\"&lt;b&gt;Model Settings&lt;/b&gt;\")\n        self._card = pn.Card(\n            load_row,\n            config_row,\n            system_input,\n            load_progress,\n            header=self._card_header,\n            sizing_mode=\"stretch_width\",\n            margin=(5, 20, 5, 0),\n            align=\"center\",\n        )\n        self._model_select.param.watch(self._update_model_slug, \"value\")\n        if pn.state.location:\n            pn.state.location.sync(self, {\"model_slug\": \"model_slug\"})\n\n    def _get_model_options(self, model_mapping):\n        \"\"\"\n        Generates the model options for the nested select widget.\n\n        Args:\n            model_mapping (dict):\n                A dictionary mapping model names to parameters and quantizations.\n\n        Returns\n        -------\n            dict: A dictionary representing the model options.\n        \"\"\"\n        model_options = {\n            model_name: {parameters: list(quantizations.keys()) for parameters, quantizations in model_mapping[model_name].items()}\n            for model_name in sorted(model_mapping)\n        }\n        return model_options\n\n    @param.depends(\"model_mapping\", watch=True, on_init=True)\n    def _update_model_select(self):\n        \"\"\"Updates the model selection widget when the model_mapping changes.\"\"\"\n        options = self._get_model_options(self.model_mapping)\n        levels = [\n            {\"name\": \"Model\", \"sizing_mode\": \"stretch_width\"},\n            {\"name\": \"Size\", \"sizing_mode\": \"stretch_width\"},\n            {\"name\": \"Quantization\", \"sizing_mode\": \"stretch_width\"},\n        ]\n        value = None\n        if self.model_slug:\n            model_params = ModelParam.from_model_slug(self.model_slug)\n            value = model_params.to_dict(levels)\n        self._model_select.param.update(\n            options=options,\n            levels=levels,\n            value=value,\n        )\n        self.param[\"model_slug\"].objects = sorted(value for models in MODEL_MAPPING.values() for sizes in models.values() for value in sizes.values())\n\n    def _update_model_slug(self, event):\n        \"\"\"\n        Updates the model_slug parameter based on the selected model.\n\n        Args:\n            event (param.parameterized.Event):\n                A change event from the model selection widget.\n        \"\"\"\n        self.model_slug = ModelParam.from_nested_select(self._model_select).lookup_model_slug(self.model_mapping)\n\n    @param.depends(\"model_slug\", watch=True)\n    def _update_nested_select(self):\n        \"\"\"Updates the nested select widget when the model slug changes.\"\"\"\n        model_param = ModelParam.from_model_slug(self.model_slug)\n        self._model_select.value = model_param.to_dict(self._model_select.levels)\n\n    @param.depends(\"load_model\", watch=True)\n    def _load_model(self):\n        \"\"\"Loads the model when the load_model event is triggered.\"\"\"\n        if self.model_slug in self._card_header.object:\n            return\n        self.load_status = {\n            \"progress\": 0,\n            \"text\": f\"Preparing to load {self.model_slug}\",\n        }\n        self._send_msg({\"type\": \"load\"})\n\n    @param.depends(\"multiple_loads\", watch=True)\n    def _on_multiple_loads(self):\n        if not self.multiple_loads and self.loaded:\n            self._card.visible = False\n\n    @param.depends(\"loading\", watch=True)\n    def _on_loading(self):\n        self._model_select.disabled = self.loading\n\n    @param.depends(\"loaded\", watch=True)\n    def _on_loaded(self):\n        if self.loaded:\n            self._card.collapsed = True\n            if not self.multiple_loads:\n                self._card.visible = False\n            self._load_button.disabled = True\n            self._card_header.object = f\"Model Settings (Loaded: {self.model_slug})\"\n        else:\n            self._card.visible = True\n            self._load_button.disabled = False\n\n    @param.depends(\"model_slug\", watch=True)\n    def _on_model_slug(self):\n        self.loaded = False\n\n    def _handle_msg(self, msg):\n        \"\"\"\n        Handles messages from the WebLLM.\n\n        Args:\n            msg (dict):\n                The message data received from the WebLLM.\n        \"\"\"\n        if self.running:\n            self._buffer.insert(0, msg)\n\n    async def create_completion(self, messages):\n        \"\"\"\n        Creates a chat completion with the WebLLM.\n\n        Args:\n            messages (list):\n                A list of message dictionaries representing the chat history.\n\n        Yields\n        ------\n             dict:  The response chunks from the LLM.\n\n        Raises\n        ------\n            RuntimeError: If the model is not loaded.\n        \"\"\"\n        self._send_msg({\"type\": \"completion\", \"messages\": messages})\n        while True:\n            await asyncio.sleep(0.01)\n            if not self._buffer:\n                continue\n            choice = self._buffer.pop()\n            yield choice\n            reason = choice[\"finish_reason\"]\n            if reason == \"error\":\n                raise RuntimeError(\"Model not loaded\")\n            elif reason:\n                return\n\n    def refresh_model_mapping(self):\n        \"\"\"\n        Refreshes the model mapping by fetching the latest from the mlc.ai website.\n\n        This method scrapes the mlc.ai website to get the available models and their\n        parameters.\n\n        Requires bs4 and requests to be installed.\n        \"\"\"\n        import bs4\n        import requests  # type: ignore\n\n        text = requests.get(\"https://mlc.ai/models#mlc-models\").text\n        soup = bs4.BeautifulSoup(text, \"html.parser\")\n        table = soup.find(\"table\")\n        links = table.find_all(\"a\")\n        model_mapping: dict = {}\n        for link in links:\n            model_slug = link.get(\"href\").rsplit(\"/\", 1)[-1]\n            model_params = ModelParam.from_slug(model_slug)\n            model_name = model_params.model\n            model_parameters = model_params.size\n            model_quantization = model_params.quantization\n            if model_name not in model_mapping:\n                model_mapping[model_name] = {}\n            if model_parameters not in model_mapping[model_name]:\n                model_mapping[model_name][model_parameters] = {}\n            model_mapping[model_name][model_parameters][model_quantization] = model_slug\n        self.model_mapping = model_mapping\n\n    async def callback(self, contents: str, user: str, instance: ChatInterface):\n        \"\"\"\n        Callback function for chat completion.\n\n        Args:\n            contents (str):\n                The current user message.\n            user (str):\n                The username of the user sending the message.\n            instance (ChatInterface):\n                The ChatInterface instance.\n\n        Yields\n        ------\n            dict or str: Yields either the messages as dict or a markdown string\n\n        Raises\n        ------\n            RuntimeError: If the model is not loaded\n        \"\"\"\n        if not self.loaded:\n            return\n        self.running = False\n        self._buffer.clear()\n\n        messages = [{\"role\": \"system\", \"content\": self.system}] + instance.serialize(limit=self.history)\n\n        message = \"\"\n        async for chunk in self.create_completion(messages):\n            message += chunk[\"delta\"].get(\"content\", \"\")\n            yield message\n\n    @property\n    def menu(self):\n        \"\"\"\n        Returns the model selection widget.\n\n        Returns\n        -------\n            pn.widgets.NestedSelect: The model selection widget.\n        \"\"\"\n        return self._card\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.history","title":"<code>history = param.Integer(default=10, bounds=(1, 100), doc='The number of previous messages to include in the completion.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.load_layout","title":"<code>load_layout = param.Selector(default='row', objects=['row', 'column'], doc='\\n        The layout type of the widgets.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.load_model","title":"<code>load_model = param.Event(doc='Event to trigger model loading.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.load_status","title":"<code>load_status = param.Dict(default={'text': '', 'progress': 0})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.loaded","title":"<code>loaded = param.Boolean(default=False, doc='Whether the model is loaded.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.loading","title":"<code>loading = param.Boolean(default=False, doc='\\n        Whether the model is currently loading.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.menu","title":"<code>menu</code>  <code>property</code>","text":"<p>Returns the model selection widget.</p>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.menu--returns","title":"Returns","text":"<pre><code>pn.widgets.NestedSelect: The model selection widget.\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.model_mapping","title":"<code>model_mapping = param.Dict(default=MODEL_MAPPING, doc='Nested mapping of model names to slugs.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.model_slug","title":"<code>model_slug = param.Selector(default='', doc='The model slug to load.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.multiple_loads","title":"<code>multiple_loads = param.Boolean(default=True, doc='Whether to allow loading different models multiple times')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.running","title":"<code>running = param.Boolean(default=False, doc='Whether the LLM is currently running.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.system","title":"<code>system = param.String(default='Be the most you can be; spread positivity!', doc='The system prompt for the model completion.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.temperature","title":"<code>temperature = param.Number(default=1, bounds=(0, 2), doc='The temperature for the model completion.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.callback","title":"<code>callback(contents, user, instance)</code>  <code>async</code>","text":"<p>Callback function for chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>str</code> <p>The current user message.</p> required <code>user</code> <code>str</code> <p>The username of the user sending the message.</p> required <code>instance</code> <code>ChatInterface</code> <p>The ChatInterface instance.</p> required"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.callback--yields","title":"Yields","text":"<pre><code>dict or str: Yields either the messages as dict or a markdown string\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.callback--raises","title":"Raises","text":"<pre><code>RuntimeError: If the model is not loaded\n</code></pre> Source code in <code>src/panel_web_llm/main.py</code> <pre><code>async def callback(self, contents: str, user: str, instance: ChatInterface):\n    \"\"\"\n    Callback function for chat completion.\n\n    Args:\n        contents (str):\n            The current user message.\n        user (str):\n            The username of the user sending the message.\n        instance (ChatInterface):\n            The ChatInterface instance.\n\n    Yields\n    ------\n        dict or str: Yields either the messages as dict or a markdown string\n\n    Raises\n    ------\n        RuntimeError: If the model is not loaded\n    \"\"\"\n    if not self.loaded:\n        return\n    self.running = False\n    self._buffer.clear()\n\n    messages = [{\"role\": \"system\", \"content\": self.system}] + instance.serialize(limit=self.history)\n\n    message = \"\"\n    async for chunk in self.create_completion(messages):\n        message += chunk[\"delta\"].get(\"content\", \"\")\n        yield message\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.create_completion","title":"<code>create_completion(messages)</code>  <code>async</code>","text":"<p>Creates a chat completion with the WebLLM.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries representing the chat history.</p> required"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.create_completion--yields","title":"Yields","text":"<pre><code> dict:  The response chunks from the LLM.\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.create_completion--raises","title":"Raises","text":"<pre><code>RuntimeError: If the model is not loaded.\n</code></pre> Source code in <code>src/panel_web_llm/main.py</code> <pre><code>async def create_completion(self, messages):\n    \"\"\"\n    Creates a chat completion with the WebLLM.\n\n    Args:\n        messages (list):\n            A list of message dictionaries representing the chat history.\n\n    Yields\n    ------\n         dict:  The response chunks from the LLM.\n\n    Raises\n    ------\n        RuntimeError: If the model is not loaded.\n    \"\"\"\n    self._send_msg({\"type\": \"completion\", \"messages\": messages})\n    while True:\n        await asyncio.sleep(0.01)\n        if not self._buffer:\n            continue\n        choice = self._buffer.pop()\n        yield choice\n        reason = choice[\"finish_reason\"]\n        if reason == \"error\":\n            raise RuntimeError(\"Model not loaded\")\n        elif reason:\n            return\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLM.refresh_model_mapping","title":"<code>refresh_model_mapping()</code>","text":"<p>Refreshes the model mapping by fetching the latest from the mlc.ai website.</p> <p>This method scrapes the mlc.ai website to get the available models and their parameters.</p> <p>Requires bs4 and requests to be installed.</p> Source code in <code>src/panel_web_llm/main.py</code> <pre><code>def refresh_model_mapping(self):\n    \"\"\"\n    Refreshes the model mapping by fetching the latest from the mlc.ai website.\n\n    This method scrapes the mlc.ai website to get the available models and their\n    parameters.\n\n    Requires bs4 and requests to be installed.\n    \"\"\"\n    import bs4\n    import requests  # type: ignore\n\n    text = requests.get(\"https://mlc.ai/models#mlc-models\").text\n    soup = bs4.BeautifulSoup(text, \"html.parser\")\n    table = soup.find(\"table\")\n    links = table.find_all(\"a\")\n    model_mapping: dict = {}\n    for link in links:\n        model_slug = link.get(\"href\").rsplit(\"/\", 1)[-1]\n        model_params = ModelParam.from_slug(model_slug)\n        model_name = model_params.model\n        model_parameters = model_params.size\n        model_quantization = model_params.quantization\n        if model_name not in model_mapping:\n            model_mapping[model_name] = {}\n        if model_parameters not in model_mapping[model_name]:\n            model_mapping[model_name][model_parameters] = {}\n        model_mapping[model_name][model_parameters][model_quantization] = model_slug\n    self.model_mapping = model_mapping\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin","title":"<code>WebLLMComponentMixin</code>","text":"<p>               Bases: <code>Parameterized</code></p> <p>A mixin class for integrating the WebLLM component with other components.</p> <p>This mixin provides an easy way to add a WebLLM instance and its necessary attributes to other Panel components.</p> Source code in <code>src/panel_web_llm/main.py</code> <pre><code>class WebLLMComponentMixin(param.Parameterized):\n    \"\"\"\n    A mixin class for integrating the WebLLM component with other components.\n\n    This mixin provides an easy way to add a WebLLM instance and its necessary\n    attributes to other Panel components.\n    \"\"\"\n\n    model_slug = param.String(\n        doc=\"The model slug to load.\",\n    )\n\n    multiple_loads = param.Boolean(\n        default=True,\n        doc=\"Whether to allow loading different models multiple times.\",\n    )\n\n    load_on_init = param.Boolean(\n        default=False,\n        doc=\"Whether to load the model on initialization.\",\n    )\n\n    web_llm_kwargs = param.Dict(default={}, doc=\"Keyword arguments to propagate to the WebLLM.\")\n\n    def __init__(self, **params):\n        \"\"\"\n        Initializes the WebLLMComponentMixin.\n\n        Args:\n            **params:\n                Keyword arguments for the Param base classes.\n        \"\"\"\n        super().__init__(**params)\n        self.web_llm = WebLLM(\n            model_slug=self.model_slug,\n            multiple_loads=self.multiple_loads,\n            **self.web_llm_kwargs,\n        )\n        self.callback = self.web_llm.callback\n        self.header = pn.Column(self.web_llm.menu, self.web_llm)\n        pn.state.onload(self._onload)\n\n    def _onload(self):\n        if self.load_on_init:\n            self.web_llm.load_model = True\n        else:\n            self.help_text = \"Please first load the model, then start chatting.\"\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin.callback","title":"<code>callback = self.web_llm.callback</code>  <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin.header","title":"<code>header = pn.Column(self.web_llm.menu, self.web_llm)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin.load_on_init","title":"<code>load_on_init = param.Boolean(default=False, doc='Whether to load the model on initialization.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin.model_slug","title":"<code>model_slug = param.String(doc='The model slug to load.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin.multiple_loads","title":"<code>multiple_loads = param.Boolean(default=True, doc='Whether to allow loading different models multiple times.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin.web_llm","title":"<code>web_llm = WebLLM(model_slug=self.model_slug, multiple_loads=self.multiple_loads, **self.web_llm_kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMComponentMixin.web_llm_kwargs","title":"<code>web_llm_kwargs = param.Dict(default={}, doc='Keyword arguments to propagate to the WebLLM.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMFeed","title":"<code>WebLLMFeed</code>","text":"<p>               Bases: <code>ChatFeed</code>, <code>WebLLMComponentMixin</code></p> <p>See ChatFeed for params and usage.</p> Source code in <code>src/panel_web_llm/main.py</code> <pre><code>class WebLLMFeed(ChatFeed, WebLLMComponentMixin):\n    \"\"\"See [ChatFeed](https://panel.holoviz.org/reference/chat/ChatFeed.html) for params and usage.\"\"\"\n</code></pre>"},{"location":"reference/panel_web_llm/#panel_web_llm.WebLLMInterface","title":"<code>WebLLMInterface</code>","text":"<p>               Bases: <code>ChatInterface</code>, <code>WebLLMComponentMixin</code></p> <p>See ChatInterface for params and usage.</p> Source code in <code>src/panel_web_llm/main.py</code> <pre><code>class WebLLMInterface(ChatInterface, WebLLMComponentMixin):\n    \"\"\"See [ChatInterface](https://panel.holoviz.org/reference/chat/ChatInterface.html) for params and usage.\"\"\"\n</code></pre>"}]}